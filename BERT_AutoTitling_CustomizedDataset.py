# -*- coding: utf-8 -*-
"""Summary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E6CbTw-pww2yfXNzQSn5w6mSAYi7_ou6
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/ColabNotebooks/summary')

# Get train dict
#from urllib.request import urlretrieve
#url = "https://thunlp.oss-cn-qingdao.aliyuncs.com/THUCNews.zip"
#urlretrieve(url, "THUCNews.zip")

# Move
#!mv /content/THUCNews.zip /content/drive/My\ Drive/ColabNotebooks/summary/extra_dict

#Unzip train file
#import zipfile
#path = zipfile.ZipFile("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/Combine-20000.zip")
#path.extractall(path="/content/drive/My Drive/ColabNotebooks/summary/extra_dict/Train/AAA")

import torch 
from tqdm import tqdm
import torch.nn as nn 
from torch.optim import Adam
import numpy as np
import os
import json
import time
import glob
import bert_seq2seq
from torch.utils.data import Dataset, DataLoader
from bert_seq2seq.tokenizer import Tokenizer, load_chinese_base_vocab
from bert_seq2seq.utils import load_bert, load_model_params, load_recent_model

vocab_path = "/content/drive/My Drive/ColabNotebooks/summary/state_dict/bert-base-chinese-pytorch_model.txt"  # roberta模型字典的位置
word2idx, keep_tokens = load_chinese_base_vocab(vocab_path, simplfied=True)
model_name = "bert"  # 选择模型名字
model_path = "/content/drive/My Drive/ColabNotebooks/summary/state_dict/bert-base-chinese-pytorch_model.bin"  # 模型位置
recent_model_path = "/content/drive/My Drive/ColabNotebooks/summary/state_dict/1111bert-base.bin"   # 用于把已经训练好的模型继续训练
model_save_path = "/content/drive/My Drive/ColabNotebooks/summary/state_dict/1111bert-base.bin"
batch_size = 16
lr = 1e-5
maxlen = 256

def read_file(filename):
  fh = open(filename, "r")
  try:
      return fh.read()
  finally:
      fh.close()

sents_src = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.src") 
sents_tgt = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.tgt")
lines = sents_src.split('\n')
print (lines[99998])
lines = sents_tgt.split('\n')
print (lines[99998])

class BertDataset(Dataset):
    """
    针对特定数据集，定义一个相关的取数据的方式
    """
    def __init__(self) :
        ## 一般init函数是加载所有数据
        super(BertDataset, self).__init__()
        self.sents_src = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.src") 
        self.sents_tgt = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.tgt")
        self.sents_src = self.sents_src.split('\n')
        self.sents_tgt = self.sents_tgt.split('\n')

        ## 拿到所有文件名字
        self.idx2word = {k: v for v, k in word2idx.items()}
        self.tokenizer = Tokenizer(word2idx)

    def __getitem__(self, i):
        # 得到单个数据
        # print(i)
        title = self.sents_tgt[i]
        content = self.sents_src[i]
        token_ids, token_type_ids = self.tokenizer.encode( content, title, max_length=maxlen)
        output = {"token_ids": token_ids,"token_type_ids": token_type_ids,}
        return output

        self.__getitem__(i + 1)

    def __len__(self):

        data_size = len(self.sents_src)
        return data_size
        
def collate_fn(batch):
    """
    动态padding， batch为一部分sample
    """

    def padding(indice, max_length, pad_idx=0):
        """
        pad 函数
        """
        pad_indice = [item + [pad_idx] * max(0, max_length - len(item)) for item in indice]
        return torch.tensor(pad_indice)

    token_ids = [data["token_ids"] for data in batch]
    max_length = max([len(t) for t in token_ids])
    token_type_ids = [data["token_type_ids"] for data in batch]

    token_ids_padded = padding(token_ids, max_length)
    token_type_ids_padded = padding(token_type_ids, max_length)
    target_ids_padded = token_ids_padded[:, 1:].contiguous()

    return token_ids_padded, token_type_ids_padded, target_ids_padded

class Trainer:
    def __init__(self):

        # 判断是否有可用GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("device: " + str(self.device))

        # 定义模型
        self.bert_model = load_bert(word2idx, model_name=model_name)

        ## 加载预训练的模型参数
        #load_model_params(self.bert_model, model_path, keep_tokens=keep_tokens)

        ## 加载已经训练好的模型，继续训练
        load_recent_model(self.bert_model, recent_model_path)

        # 列印模型，将模型发送到计算设备(GPU或CPU)
        self.bert_model.to(self.device)
        print(self.bert_model) 
        # 声明需要优化的参数
        self.optim_parameters = list(self.bert_model.parameters())
        self.optimizer = torch.optim.Adam(self.optim_parameters, lr=lr, weight_decay=1e-3)
        #for name, param in self.bert_model.named_parameters():
        #  if "decoder" not in name: # classifier layer
        #    param.requires_grad = False
        #  print (name, param.requires_grad)
          
        # 声明自定义的数据加载器
  
        dataset = BertDataset()
        self.dataloader =  DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

    def train(self, epoch):

        # 一个epoch的训练
        self.bert_model.train()
        self.iteration(epoch, dataloader=self.dataloader, train=True)
    
    def save(self, save_path):
        """
        保存模型
        """
        torch.save(self.bert_model.state_dict(), save_path)
        print("{} saved!".format(save_path))

    def iteration(self, epoch, dataloader, train=True):
        total_loss = 0
        start_time = time.time() ## 得到当前时间
        step = 0
        report_loss = 0
        for token_ids, token_type_ids, target_ids in tqdm(dataloader,position=0, leave=True):
            step += 1
            if step % 1000 == 0:
                self.bert_model.eval()
                test_data = [
                 "人工智慧/人工智能（Artificial Intelligence, AI [Wiki , 1]），是指由人製造出來的機器所表現出來的智慧。通常是指電腦模擬/模擬人類思維過程以模仿人類能力或行為的能力。由上述定義可知，人工智慧這個題目應該早在有計算機出來的時候，應該就有這個名詞的出現了，所以人工智慧這個名詞出現的非常早(早於1956年[1])。但因為早期計算機/電腦的效能和限制，因此只能用來解決一些簡單的問題，無法實際用在解決現實生活的問題，所以理論雖然一直有在發展，但這個主題一直被限制住，沒有蓬勃發展起來。",
                 "今天是由北大統計吳漢銘教授（很年輕很潮之感）授課，頗幽默風趣，而且還考量大家對於統計的熟悉程度，把課程內容濃縮再濃縮，簡化再簡化，真的辛苦了。雖然偶爾還是會堅持要講重要公式推導，但都會即時拉回並自嘲 「再講下去會進入迷航空間」（雖然我覺得是彌留）。授課資料幾乎都上網公開，還包含完整R程式碼，完全不怕被偷學，強者風範。一開始先介紹為什麼推薦R，原因是R的強項在資料分析（Python是資料處理），目前已有約1萬多種套件，TIOBE排名也大幅躍升。一般進行R開發專案，還會另外搭配IDE RStudio。吳提到其實工具不是優先考量的重點，因為不管事學術或產業，很少只用一個工具。",
                 "在做產品時，常常會有個衝動想要挑戰大市場，因為市場等於錢呀！老闆很容易就會想：「這個市場有這麼多目標用戶，在這當中只要能做到1%的市場，我就賺飽了，1%，輕輕鬆鬆吧！」問題就在於，用戶選擇產品的決策，常常選的是「最」符合需求、「最」能解決她們問題、相似的產品之中「最」便宜的，或是品牌知名度「最」高，他們用起來最安心的。因為消費者只需要購買「一個」產品，所以會選擇的是他們手中「前幾名」的選項，而不是「各方面都差強人意」的選項，也就是說，大部分的市場佔有率，並不是「平均分配」，而是有「馬太效應」，前幾名的品牌會吸收大部分的用戶，剩下的品牌很大部分都乏人問津。",
                 ]
                for text in test_data:
                    print(self.bert_model.generate(text, beam_size=3,device=self.device))
                print("loss is " + str(report_loss))
                report_loss = 0
                
                # self.eval(epoch)
                self.bert_model.train()

            if step % 1000 == 0:
                self.save(model_save_path)

            token_ids = token_ids.to(self.device)
            token_type_ids = token_type_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # 因为传入了target标签，因此会计算loss并且返回
            predictions, loss = self.bert_model(token_ids,
                                                token_type_ids,
                                                labels=target_ids,
                                                device=self.device
                                                )
            report_loss += loss.item()
            # 反向传播
            if train:
                # 清空之前的梯度
                self.optimizer.zero_grad()
                # 反向传播, 获取新的梯度
                loss.backward()
                # 用获取的梯度更新模型参数
                self.optimizer.step()

            # 为计算当前epoch的平均loss
            total_loss += loss.item()

        end_time = time.time()
        spend_time = end_time - start_time
        # 打印训练信息
        print("epoch is " + str(epoch)+". loss is " + str(total_loss) + ". spend time is "+ str(spend_time))
        # 保存模型
        self.save(model_save_path)

# 测试一下自定义数据集
    
    #vocab_path = "/content/drive/My Drive/ColabNotebooks/summary/state_dict/vocab.txt" 
    #sents_src = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.src") 
    #sents_tgt = read_file ("/content/drive/My Drive/ColabNotebooks/summary/extra_dict/train.tgt")

    # sents_src, sents_tgt = read_file ("./corpus/auto_title/train.src", "./corpus/auto_title/train.tgt")
    # sents_src= torch.load("./corpus/auto_title/train_clean.src")
    # sents_tgt = torch.load("./corpus/auto_title/train_clean.tgt")
    
    #import time
    #dataset = BertDataset(sents_src, sents_tgt, vocab_path)
    #word2idx = load_chinese_base_vocab(vocab_path)
    #tokenier = Tokenizer(word2idx)
    #dataloader =  DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    
    # for token_ids, token_type_ids, target_ids in dataloader:
    #     # print(token_ids.shape)
    #     print(tokenier.decode(token_ids[0].tolist()))
    #     print(tokenier.decode(token_ids[1].tolist()))
    #     # print(token_type_ids)
    #     # print(target_ids.shape)
    #     # print(tokenier.decode(target_ids[0].tolist()))
    #     # print(tokenier.decode(target_ids[1].tolist()))
    #     break

    # src, tgt = read_file("./corpus/auto_title/train.src", "./corpus/auto_title/train.tgt")
    # save_src, save_tgt = [], []
    # for src_i, tgt_i in zip(src, tgt):
    #     src_i = src_i.replace("“", "").replace("”", "").replace("——", "-").replace("—", "-")
    #     tgt_i = tgt_i.replace("“", "").replace("”", "").replace("——", "-").replace("—", "-")

    #     save_src.append(src_i)
    #     save_tgt.append(tgt_i)

    # torch.save(save_src, "./corpus/auto_title/train_clean.src")
    # torch.save(save_tgt, "./corpus/auto_title/train_clean.tgt")

import gc 
gc.collect() 

import torch
torch.cuda.empty_cache()

if __name__ == '__main__':

    trainer = Trainer()
   
    train_epoches = 5

    for epoch in range(train_epoches):
        # 训练一个epoch
        trainer.train(epoch)