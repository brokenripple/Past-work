# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDIzCXV5XLY07OEW1okJzIW1jcjWKOe1
"""

from urllib.request import urlretrieve
url = "https://github.com/Elwing-Chou/ml0930/raw/master/titanic/train.csv"
urlretrieve(url, "train.csv")
url = "https://github.com/Elwing-Chou/ml0930/raw/master/titanic/test.csv"
urlretrieve(url, "test.csv")

import pandas as pd
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

datas = pd.concat([train_df, test_df], axis=0, ignore_index=True)
datas = datas.drop(["PassengerId", "Survived"], axis=1)

na = datas.isna ().sum()
na

"""看種類補缺失"""

na = datas.isna ().sum() # see if data is missing
# have something missing Series [True/False list/series]
na[na > 0].sort_values(ascending=False)

most = datas["Embarked"].value_counts().idxmax()
datas["Embarked"] = datas["Embarked"].fillna(most)

na = datas.isna ().sum()
na

#different kind of ticket number with different numbers
dic = datas["Ticket"].value_counts()
print (dic)
def count (n):
  return dic[n]
datas["Ticket"] = datas["Ticket"].apply(count)
datas["Ticket"]

# 數字 (Age) 通常用中位數補 (sklearn.impute > 用趨勢補)，數字類別 (Pclass) 不能用中位數補喔。
med = datas.median ().drop(["Pclass"]) # Age and Fare
datas = datas.fillna(med)

# 補 Cabin ，沒有的寫 None
def cabin (c) : 
  if pd.isna (c):
    return None
  else:
    return c[0]

datas["Cabin"] = datas["Cabin"].apply(cabin)

# One-hot encoding 類別，轉成多欄數字(有大小關係或只有兩種可以不做，兩種轉成二值行)

def name(n):
  mid = n.split(",")[-1].split(".")[0]
  mid = mid.strip()
  return mid
middle = datas["Name"].apply(name)
counts = middle.value_counts()
reserved = counts [:4].index
def name2(title):
  if title in reserved:
    return title
  else:
    return None

datas["Name"] = middle.apply(name2)

datas

datas = pd.get_dummies(datas) # One-hot encoding for none-number items
datas = pd.get_dummies(datas, columns=["Pclass"])

datas # 增加欄位不一定增加命中率但至少不會變爛
datas ["Family"] = datas ["SibSp"] + datas ["Parch"]
datas

# Random Forest > A lot of decision trees to vote   
# Cross validation 1st 0~8 training 9 test, 2nd 0~7+9 train 8 test a2

"""class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)

sklearn.model_selection.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)[source]¶
"""

# [firt row, second row]
x_train = datas.iloc[:len(train_df)]
x_predict = datas.iloc [len(train_df):]
y_train = train_df ["Survived"]
x_predict

x_train

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

clf = RandomForestClassifier (random_state=1)
params = {
    "n_estimators": range(20, 100, 5), # 80 trees
    "max_depth": range (4,11)
}
# RandomForestClassifier 找最佳
cv = GridSearchCV (clf, params, cv=10, n_jobs=-1 ) 
cv.fit(x_train,y_train)
print (cv.best_score_)
print (cv.best_params_)

# Random Status 微調
from sklearn.model_selection import cross_val_score
import numpy as np

clf = RandomForestClassifier (n_estimators= 60, max_depth = 7, random_state=None)
scores = cross_val_score (clf,x_train,y_train, cv=10, n_jobs=-1)
print (scores)
print (np.average(scores))

clf = RandomForestClassifier(n_estimators=61, max_depth=7)
clf.fit (x_train,y_train)
pre = clf.predict (x_predict)
df = pd.DataFrame({
    "PassengerId": test_df["PassengerId"],
    "Survived": pre
})
df.to_csv("rf.csv", encoding= "utf-8", index=False)

#For random forest

clf.estimators_[0]
clf.feature_importances_
pd.DataFrame ({"columns": x_train.columns,"importance": clf.feature_importances_}).sort_values("importance", axis=0, ascending= False)
#重要 可以看 features importances

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(20,8))
plt.subplot(1,2,1)
sns.countplot(train_df["Sex"])
plt.subplot(1,2,2)
sns.countplot(x=train_df["Sex"], hue=train_df["Survived"])

plt.figure(figsize=(20,8))

sns.distplot (train_df["Age"], kde=True)

plt.figure(figsize=(20,20))
c = pd.cut(train_df["Age"], bins =10) #bins > 分幾段 (包含) [不包含]
plt.subplot(2,1,2)
sns.countplot(x=c, hue =train_df["Survived"])
plt.xticks (rotation = 45)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
datas_scale = scaler.fit_transform(datas)
datas_scale = pd.DataFrame(datas_scale, columns=datas.columns)
x_train_scale = datas_scale.iloc[:len(train_df)]
x_predict_scale = datas_scale.iloc[len(train_df):]
x_predict_scale

from sklearn.neighbors import KNeighborsClassifier
params = {
    "n_neighbors":range(5, 100),
}
clf = KNeighborsClassifier()
cv = GridSearchCV(clf, params, cv=10, n_jobs=-1)
cv.fit(x_train_scale, y_train)
print(cv.best_score_)
print(cv.best_params_)

clf = KNeighborsClassifier(n_neighbors = 11)
clf.fit (x_train_scale,y_train)
pre = clf.predict (x_predict_scale)
df = pd.DataFrame({
    "PassengerId": test_df["PassengerId"],
    "Survived": pre
})
df.to_csv("knn.csv", encoding= "utf-8", index=False)